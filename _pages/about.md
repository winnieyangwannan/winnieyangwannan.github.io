---
layout: about
title: Home
permalink: /
subtitle: 

profile:
  align: right
  image: prof_pic.png
  image_circular: false # crops the image to make it circular
  more_info: >


news: false # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: ture # includes social icons at the bottom of the page
---
# Wannan (Winnie) Yang
\
&nbsp;

###  Welcome to My Corner of the Web! ðŸ‘‹

I'm Winnie, a PhD candidate at New York University (NYU)'s [Buzsakilab](https://buzsakilab.com/wp/publications/),
where I decode the content of 'dream' (replay) to unravel the mechanism of
[memory consolidation during sleep](https://winnieyangwannan.github.io/RippleTagging/) (my work was recently published in
[Science](https://www.science.org/doi/10.1126/science.adk8261) ðŸ¥³).

### From Neuroscience to AI
In early 2024 I was beginning my transition from Neuroscience to AI. By then, progress in AI for several years had already exploded in a myriad of promise and hope. But what struck me in 2024 was that the **rate** of progress was continuing unabated, and my feelings of open-ended benefit for humanity turned to worries for our extinction as we barrel unprepared toward the direction of ASI. My main motivation is that in order for humanity to benefit from the plethora of benefits that AI brings, we _must make sure the system is aligned_, and we must build such safe systems **NOW**. This is a goal which I am wholly committed as my lifeâ€™s work.

Within the broad goal of helping superalignment, my current research interests revolve around building automated alignment research assistants -- perhaps by better leveraging interpretability tools to monitor their internals.  One area, for instance, that I am currently excited about to is to monitor/evaluate the trustworthiness / deception of both alignment assistant (supervisor) and the supervisee via interpretability tools. Delving into the black box is within my existing expertise as a neuroscientist (expertise in â€˜internal oversightâ€™ of the brain) as well as my recent research experience in LLM deception. 

### Current Focus
I've started the exciting journey of transitioning from neuroscience to AI research. My current interests include:

- [Monitoring and steering LLMs to be honest](https://winnieyangwannan.github.io/LLM_Deception/) 
- Scalable Oversight
- Adversarial robustness

---


## *NEWS*{: style="color: #5663b0;" }

#####  *- [2025-01-08]: Our paper titled "Hippocampal neuronal activity is aligned with action plans" was published in [Nature](https://www.nature.com/articles/s41586-024-08397-7). ðŸŽ‰*{: style="color: #5663b0;" }



#####  *- [2024-10-12]: Our 'LLM lying' paper was accepted at the SafeGenAi workshop at [NeurIPS](https://winnieyangwannan.github.io/LLM_Deception/). ðŸŽ‰*{: style="color: #5663b0;" }



#####  *- [2024-10-01]: Our 'LLM lying' paper was submitted to [ICLR](https://winnieyangwannan.github.io/LLM_Deception/).*{: style="color: #5663b0;" }



#####  *- [2024-05-21]: Our 'ripple-tagging' paper was covered in the [Quanta Magzine](https://www.quantamagazine.org/electric-ripples-in-the-resting-brain-tag-memories-for-storage-20240521/).*{: style="color: #5663b0;" }



#####  *- [2024-03-28]: Our 'ripple-tagging' paper was published in [Science](https://www.science.org/doi/10.1126/science.adk8261).ðŸŽ‰*{: style="color: #5663b0;" }
